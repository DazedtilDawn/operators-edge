#!/usr/bin/env python3
"""
Operator's Edge v7.0 - Pattern & Rule Metrics
Measure the impact of decision-time interventions.

Metrics tracked:
1. Pattern surface events (when, what, confidence) [v6.0 legacy]
2. Edit outcomes (success/failure with/without patterns) [v6.0 legacy]
3. Reinforcement history (graduation velocity) [v6.0 legacy]
4. Session efficiency (steps, blocks, mismatches) [v6.0 legacy]
5. Rule effectiveness (fires, outcomes, overrides) [v7.0 NEW]
6. Outcome correlation (rule â†’ success/failure) [v7.0 NEW]
"""
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional
import json
import os
from pathlib import Path


@dataclass
class PatternSurfaceEvent:
    """Record of a pattern being surfaced."""
    timestamp: str
    file_path: str
    context: str
    patterns_count: int
    pattern_types: List[str]
    confidence_levels: List[str]
    objective: str = ""
    step_description: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return {
            "timestamp": self.timestamp,
            "file_path": self.file_path,
            "context": self.context[:100],
            "patterns_count": self.patterns_count,
            "pattern_types": self.pattern_types,
            "confidence_levels": self.confidence_levels,
            "objective": self.objective[:50],
            "step_description": self.step_description[:50],
        }


@dataclass
class EditOutcomeEvent:
    """Record of an edit outcome for correlation analysis."""
    timestamp: str
    file_path: str
    success: bool
    patterns_shown: int
    patterns_reinforced: int

    def to_dict(self) -> Dict[str, Any]:
        return {
            "timestamp": self.timestamp,
            "file_path": self.file_path,
            "success": self.success,
            "patterns_shown": self.patterns_shown,
            "patterns_reinforced": self.patterns_reinforced,
        }


def get_metrics_file() -> Path:
    """Get the path to the metrics file."""
    proof_dir = Path(os.environ.get("CLAUDE_PROJECT_DIR", ".")) / ".proof"
    proof_dir.mkdir(exist_ok=True)
    return proof_dir / "pattern_metrics.jsonl"


def log_pattern_surface(
    file_path: str,
    context: str,
    patterns: List[Any],
    objective: str = "",
    step_description: str = ""
) -> None:
    """Log a pattern surface event."""
    event = PatternSurfaceEvent(
        timestamp=datetime.now().isoformat(),
        file_path=file_path,
        context=context,
        patterns_count=len(patterns),
        pattern_types=[p.type.value for p in patterns],
        confidence_levels=[p.confidence for p in patterns],
        objective=objective,
        step_description=step_description,
    )

    try:
        metrics_file = get_metrics_file()
        with open(metrics_file, "a") as f:
            f.write(json.dumps({"type": "surface", **event.to_dict()}) + "\n")
    except Exception:
        pass  # Don't fail if metrics can't be written


def log_edit_outcome(
    file_path: str,
    success: bool,
    patterns_shown: int,
    patterns_reinforced: int
) -> None:
    """Log an edit outcome event."""
    event = EditOutcomeEvent(
        timestamp=datetime.now().isoformat(),
        file_path=file_path,
        success=success,
        patterns_shown=patterns_shown,
        patterns_reinforced=patterns_reinforced,
    )

    try:
        metrics_file = get_metrics_file()
        with open(metrics_file, "a") as f:
            f.write(json.dumps({"type": "outcome", **event.to_dict()}) + "\n")
    except Exception:
        pass


def load_metrics() -> List[Dict[str, Any]]:
    """Load all metrics events."""
    metrics_file = get_metrics_file()
    if not metrics_file.exists():
        return []

    events = []
    try:
        with open(metrics_file) as f:
            for line in f:
                line = line.strip()
                if line:
                    events.append(json.loads(line))
    except Exception:
        pass

    return events


def compute_metrics_summary() -> Dict[str, Any]:
    """Compute summary statistics from metrics."""
    events = load_metrics()

    if not events:
        return {
            "status": "no_data",
            "message": "No metrics collected yet",
        }

    # Separate event types
    surface_events = [e for e in events if e.get("type") == "surface"]
    outcome_events = [e for e in events if e.get("type") == "outcome"]

    # 1. Pattern Surface Rate
    surface_rate = {
        "total_surfaces": len(surface_events),
        "avg_patterns_per_surface": (
            sum(e.get("patterns_count", 0) for e in surface_events) / len(surface_events)
            if surface_events else 0
        ),
    }

    # 2. Pattern Relevance (confidence distribution)
    all_confidences = []
    for e in surface_events:
        all_confidences.extend(e.get("confidence_levels", []))

    relevance = {
        "total_patterns_surfaced": len(all_confidences),
        "high_confidence": all_confidences.count("high"),
        "medium_confidence": all_confidences.count("medium"),
        "low_confidence": all_confidences.count("low"),
        "high_confidence_rate": (
            all_confidences.count("high") / len(all_confidences)
            if all_confidences else 0
        ),
    }

    # 3. Edit Success Rate (with vs without patterns)
    edits_with_patterns = [e for e in outcome_events if e.get("patterns_shown", 0) > 0]
    edits_without_patterns = [e for e in outcome_events if e.get("patterns_shown", 0) == 0]

    success_with = sum(1 for e in edits_with_patterns if e.get("success", False))
    success_without = sum(1 for e in edits_without_patterns if e.get("success", False))

    edit_success = {
        "total_edits": len(outcome_events),
        "edits_with_patterns": len(edits_with_patterns),
        "edits_without_patterns": len(edits_without_patterns),
        "success_rate_with_patterns": (
            success_with / len(edits_with_patterns)
            if edits_with_patterns else None
        ),
        "success_rate_without_patterns": (
            success_without / len(edits_without_patterns)
            if edits_without_patterns else None
        ),
    }

    # 4. Pattern Types Distribution
    all_types = []
    for e in surface_events:
        all_types.extend(e.get("pattern_types", []))

    type_distribution = {}
    for t in set(all_types):
        type_distribution[t] = all_types.count(t)

    return {
        "status": "ok",
        "collection_period": {
            "first_event": events[0].get("timestamp") if events else None,
            "last_event": events[-1].get("timestamp") if events else None,
            "total_events": len(events),
        },
        "surface_rate": surface_rate,
        "relevance": relevance,
        "edit_success": edit_success,
        "pattern_types": type_distribution,
    }


def format_metrics_report(summary: Dict[str, Any]) -> str:
    """Format metrics summary as a readable report."""
    if summary.get("status") == "no_data":
        return "ðŸ“Š **Pattern Metrics**: No data collected yet."

    lines = ["### ðŸ“Š Pattern Impact Metrics", ""]

    # Collection period
    period = summary.get("collection_period", {})
    lines.append(f"**Data Collection**: {period.get('total_events', 0)} events")
    if period.get("first_event"):
        lines.append(f"  From: {period['first_event'][:10]} to {period['last_event'][:10]}")
    lines.append("")

    # Surface Rate
    sr = summary.get("surface_rate", {})
    lines.append(f"**Pattern Surfacing**:")
    lines.append(f"  - Total surface events: {sr.get('total_surfaces', 0)}")
    lines.append(f"  - Avg patterns per event: {sr.get('avg_patterns_per_surface', 0):.1f}")
    lines.append("")

    # Relevance
    rel = summary.get("relevance", {})
    if rel.get("total_patterns_surfaced", 0) > 0:
        lines.append(f"**Pattern Relevance**:")
        lines.append(f"  - High confidence: {rel.get('high_confidence', 0)} ({rel.get('high_confidence_rate', 0):.0%})")
        lines.append(f"  - Medium confidence: {rel.get('medium_confidence', 0)}")
        lines.append(f"  - Low confidence: {rel.get('low_confidence', 0)}")
        lines.append("")

    # Edit Success Correlation
    es = summary.get("edit_success", {})
    if es.get("total_edits", 0) > 0:
        lines.append(f"**Edit Success Correlation**:")
        lines.append(f"  - Total edits tracked: {es.get('total_edits', 0)}")

        with_rate = es.get("success_rate_with_patterns")
        without_rate = es.get("success_rate_without_patterns")

        if with_rate is not None:
            lines.append(f"  - With patterns: {es.get('edits_with_patterns', 0)} edits, {with_rate:.0%} success")
        if without_rate is not None:
            lines.append(f"  - Without patterns: {es.get('edits_without_patterns', 0)} edits, {without_rate:.0%} success")

        if with_rate is not None and without_rate is not None:
            diff = with_rate - without_rate
            if diff > 0.05:
                lines.append(f"  - ðŸ“ˆ Patterns correlated with +{diff:.0%} success rate")
            elif diff < -0.05:
                lines.append(f"  - ðŸ“‰ Patterns correlated with {diff:.0%} success rate")
            else:
                lines.append(f"  - âž¡ï¸ No significant correlation detected")
        lines.append("")

    # Pattern Types
    types = summary.get("pattern_types", {})
    if types:
        lines.append(f"**Pattern Types Used**:")
        for t, count in sorted(types.items(), key=lambda x: x[1], reverse=True):
            lines.append(f"  - {t}: {count}")

    return "\n".join(lines)


# =============================================================================
# LESSON LIFECYCLE METRICS
# =============================================================================

def compute_lesson_metrics(state: Dict[str, Any]) -> Dict[str, Any]:
    """Compute metrics about lesson lifecycle and graduation."""
    memory = state.get("memory", [])

    if not memory:
        return {"status": "no_lessons"}

    # Lifecycle distribution
    from pattern_engine import get_lesson_lifecycle_stage

    lifecycle_counts = {"new": 0, "established": 0, "graduated": 0, "evergreen": 0}
    reinforcement_values = []

    for m in memory:
        if isinstance(m, dict):
            stage = get_lesson_lifecycle_stage(m)
            lifecycle_counts[stage] = lifecycle_counts.get(stage, 0) + 1
            reinforcement_values.append(m.get("reinforced", 0))

    return {
        "status": "ok",
        "total_lessons": len(memory),
        "lifecycle_distribution": lifecycle_counts,
        "reinforcement_stats": {
            "min": min(reinforcement_values) if reinforcement_values else 0,
            "max": max(reinforcement_values) if reinforcement_values else 0,
            "avg": sum(reinforcement_values) / len(reinforcement_values) if reinforcement_values else 0,
        },
        "graduation_rate": (
            lifecycle_counts["graduated"] / len(memory)
            if memory else 0
        ),
    }


def format_lesson_metrics(metrics: Dict[str, Any]) -> str:
    """Format lesson metrics as readable report."""
    if metrics.get("status") == "no_lessons":
        return "ðŸ“š **Lesson Metrics**: No lessons in memory."

    lines = ["### ðŸ“š Lesson Lifecycle Metrics", ""]

    lines.append(f"**Total Lessons**: {metrics.get('total_lessons', 0)}")
    lines.append("")

    # Lifecycle distribution
    lc = metrics.get("lifecycle_distribution", {})
    lines.append("**Lifecycle Distribution**:")
    lines.append(f"  - ðŸŒ± New (learning): {lc.get('new', 0)}")
    lines.append(f"  - ðŸŒ¿ Established (proven): {lc.get('established', 0)}")
    lines.append(f"  - ðŸŽ“ Graduated (internalized): {lc.get('graduated', 0)}")
    lines.append(f"  - ðŸŒ² Evergreen (always relevant): {lc.get('evergreen', 0)}")
    lines.append("")

    # Reinforcement stats
    rs = metrics.get("reinforcement_stats", {})
    lines.append("**Reinforcement Stats**:")
    lines.append(f"  - Average: {rs.get('avg', 0):.1f}")
    lines.append(f"  - Range: {rs.get('min', 0)} - {rs.get('max', 0)}")
    lines.append("")

    # Graduation rate
    grad_rate = metrics.get("graduation_rate", 0)
    if grad_rate > 0.3:
        lines.append(f"ðŸ“ˆ **Graduation Rate**: {grad_rate:.0%} (high retention)")
    elif grad_rate > 0.1:
        lines.append(f"âž¡ï¸ **Graduation Rate**: {grad_rate:.0%} (moderate)")
    else:
        lines.append(f"ðŸŒ± **Graduation Rate**: {grad_rate:.0%} (still learning)")

    return "\n".join(lines)


# =============================================================================
# v7.0 RULE METRICS (Outcome-Based)
# =============================================================================

def compute_rule_metrics() -> Dict[str, Any]:
    """
    Compute metrics about rule effectiveness from outcome tracking.

    v7.0: This is the NEW way to measure impact - based on actual outcomes,
    not just "patterns surfaced" count.
    """
    try:
        from outcome_tracker import get_all_rule_stats, analyze_rule_impact

        stats = get_all_rule_stats()
        impact = analyze_rule_impact()

        return {
            "status": "ok",
            "impact_summary": impact,
            "rule_stats": stats.get("rules", {}),
            "updated": stats.get("updated"),
        }
    except ImportError:
        return {"status": "unavailable", "message": "Outcome tracker not available"}
    except Exception as e:
        return {"status": "error", "message": str(e)}


def format_rule_metrics(metrics: Dict[str, Any]) -> str:
    """Format v7.0 rule metrics as readable report."""
    if metrics.get("status") != "ok":
        return f"ðŸ“ **Rule Metrics**: {metrics.get('message', 'No data')}"

    lines = ["### ðŸ“ Rule Effectiveness Metrics (v7.0)", ""]

    # Impact summary
    impact = metrics.get("impact_summary", {})
    lines.append("**Overall Impact**:")
    lines.append(f"  - Rules fired: {impact.get('total_rule_fires', 0)}")
    lines.append(f"  - Success rate: {impact.get('success_rate', 0):.0%}")
    lines.append(f"  - Override rate: {impact.get('total_overrides', 0)} overrides")
    if impact.get("total_overrides", 0) > 0:
        lines.append(f"  - Override failure rate: {impact.get('override_failure_rate', 0):.0%}")
    lines.append("")

    # Per-rule stats
    rule_stats = metrics.get("rule_stats", {})
    if rule_stats:
        lines.append("**Per-Rule Stats**:")
        for rule_id, stats in sorted(rule_stats.items()):
            effectiveness = stats.get("effectiveness", 0)
            fires = stats.get("times_fired", 0)
            if effectiveness >= 0.8:
                icon = "âœ…"
            elif effectiveness >= 0.5:
                icon = "âš¡"
            else:
                icon = "âš ï¸"
            lines.append(f"  {icon} **{rule_id}**: {effectiveness:.0%} effective ({fires} fires)")
    lines.append("")

    # Recommendations
    ineffective = impact.get("ineffective_rules", 0)
    effective = impact.get("highly_effective_rules", 0)
    if ineffective > 0:
        lines.append(f"ðŸ” **{ineffective} rule(s) may need review** (low effectiveness)")
    if effective > 0:
        lines.append(f"ðŸ’ª **{effective} rule(s) highly effective** (consider stricter enforcement)")

    return "\n".join(lines)


def get_combined_metrics_report() -> str:
    """
    Generate a combined metrics report for both v6.0 (legacy) and v7.0 metrics.
    """
    lines = ["# ðŸ“Š Operator's Edge Metrics Report", ""]

    # v7.0 Rule Metrics (NEW - primary focus)
    rule_metrics = compute_rule_metrics()
    lines.append(format_rule_metrics(rule_metrics))
    lines.append("")

    # v6.0 Pattern Metrics (legacy, kept for comparison)
    pattern_summary = compute_metrics_summary()
    lines.append(format_metrics_report(pattern_summary))
    lines.append("")

    return "\n".join(lines)


if __name__ == "__main__":
    # Demo
    print(get_combined_metrics_report())
